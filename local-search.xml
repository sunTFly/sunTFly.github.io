<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>RGB相机动捕系列：相机标定（三.一、暴力重建）</title>
    <link href="/2024/06/18/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%89.%E4%B8%80%E3%80%81%E6%9A%B4%E5%8A%9B%E9%87%8D%E5%BB%BA)/"/>
    <url>/2024/06/18/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%89.%E4%B8%80%E3%80%81%E6%9A%B4%E5%8A%9B%E9%87%8D%E5%BB%BA)/</url>
    
    <content type="html"><![CDATA[<p>**RGB相机动捕系列：相机标定（三.一、暴力重建）</p><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>之前一篇文章中写了一篇 <a href="https://suntfly.github.io/2024/01/22/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%89%E3%80%81%E7%9B%B8%E6%9C%BA%E9%98%B5%E9%85%8D%E5%87%86%EF%BC%89/">RGB相机动捕系列：相机标定（三、相机阵配准）</a>，其中 2.3中提到了红点三维重建，本篇将详细介绍一下红点重建的具体过程。</p><h2 id="二、暴力匹配"><a href="#二、暴力匹配" class="headerlink" title="二、暴力匹配"></a>二、暴力匹配</h2><h3 id="2-1相机匹配"><a href="#2-1相机匹配" class="headerlink" title="2.1相机匹配"></a>2.1相机匹配</h3><p>遍历所有相机，对相机进行两两相互匹配。由于已经对相机进行标定，可根据相机参数求出两相机的交会角。去除交会角过小和过大的匹配对。因为当交会角过小时，会导致重建深度的精度不够，微小的视察会产生较大的深度歧义。当交会角过大时，重建时相邻像素之间视差值过大，导致相邻像素可能被错误的匹配。</p><h3 id="2-2红点数据暴力匹配"><a href="#2-2红点数据暴力匹配" class="headerlink" title="2.2红点数据暴力匹配"></a>2.2红点数据暴力匹配</h3><p>根据相机参数计算出基础矩阵F，根据极线关系 l&#x3D;F*m，其中l为极线，F为基础矩阵，m为坐标点。可计算第一张影像上的点到极线的几何距离。当距离小于阈值的时候则添加匹配点对。</p><h3 id="2-3重建3D"><a href="#2-3重建3D" class="headerlink" title="2.3重建3D"></a>2.3重建3D</h3><p>完成匹配对之后，通过三角测量法重建出3D点，并通过计算重投影误差去除误差较大的点数据。</p><h2 id="三、融合3D"><a href="#三、融合3D" class="headerlink" title="三、融合3D"></a>三、融合3D</h2><h3 id="3-1通过2D-3D关系进行融合"><a href="#3-1通过2D-3D关系进行融合" class="headerlink" title="3.1通过2D-3D关系进行融合"></a>3.1通过2D-3D关系进行融合</h3><p>通过上述步骤可重建出粗略的3D点，现在利用2D点和3D点的关系对数据进行融合。将所有共同2D点的生成的3D数据进行线性融合，并计算重投影误差的最大值，如果最大值小于阈值则融合3D。</p>]]></content>
    
    
    
    <tags>
      
      <tag>RGB相机动捕系列 相机标定</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RGB相机动捕系列：相机标定（三、相机阵配准）</title>
    <link href="/2024/05/11/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%89%E3%80%81%E7%9B%B8%E6%9C%BA%E9%98%B5%E9%85%8D%E5%87%86%EF%BC%89/"/>
    <url>/2024/05/11/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%89%E3%80%81%E7%9B%B8%E6%9C%BA%E9%98%B5%E9%85%8D%E5%87%86%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p><strong>RGB相机动捕系列：相机标定（三、相机阵配准）<br>一、概述</strong></p><p>在上一篇博客中我们讲了相机标定的标定计算部分。但是完成标定的相机阵的参数在尺度、原点、方向上都和真实的坐标系有区别，所以需要使用配准的方式对相机参数进行变换，还原出真实的坐标系。 </p><h1 id="二、相机阵配准"><a href="#二、相机阵配准" class="headerlink" title="二、相机阵配准"></a><strong>二、相机阵配准</strong></h1><h2 id="2-1、配准杆"><a href="#2-1、配准杆" class="headerlink" title="2.1、配准杆"></a><strong>2.1、配准杆</strong></h2><p>配准杆采用T字形红光灯，具体做法是将T形杆放置在地面并保持水平。T形杆如下图所示。</p><h2 id="2-2、配准杆红点检测"><a href="#2-2、配准杆红点检测" class="headerlink" title="2.2、配准杆红点检测"></a><strong>2.2、配准杆红点检测</strong></h2><p>水平放置配准杆后打开所有相机采集一张图像，并使用RGB相机动捕系列：相机标定（一、特征提取）这篇文章  ，链接<br><a href="https://suntfly.github.io/2024/01/05/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%80%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%89/">https://suntfly.github.io/2024/01/05/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%80%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%89/</a><br>的红点提取方法对数据进行提取，下面是选取的3相机提取出来的红点示意图</p><h2 id="2-3、红点三维重建"><a href="#2-3、红点三维重建" class="headerlink" title="2.3、红点三维重建"></a><strong>2.3、红点三维重建</strong></h2><p>使用标定的相机参数重建T形杆的3D点。具体的无序点重建方法请参考后续的博客</p><h2 id="2-4、配准"><a href="#2-4、配准" class="headerlink" title="2.4、配准"></a><strong>2.4、配准</strong></h2><p>使用将原始坐标系的T字形标定杆与目标坐标系配准,再变换原始坐标系参数。下面是配准前后的相机阵和配准杆示意图。第一张图和第二张图中黄色的点为相机阵配准前的位置，分布在【0,1】空间。红色点为T形杆。第三张图绿色点为配准后的相机阵，红色点为配准杆。可以看出配准后相机参数已经还原到了真实坐标系的参数。关于配准算法请参考后续博客</p>]]></content>
    
    
    
    <tags>
      
      <tag>RGB相机动捕系列 相机标定</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RGB相机动捕系列：相机标定（二、标定计算）</title>
    <link href="/2024/04/02/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%BA%8C%E3%80%81%E6%A0%87%E5%AE%9A%E8%AE%A1%E7%AE%97%EF%BC%89%20/"/>
    <url>/2024/04/02/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%BA%8C%E3%80%81%E6%A0%87%E5%AE%9A%E8%AE%A1%E7%AE%97%EF%BC%89%20/</url>
    
    <content type="html"><![CDATA[<p><strong>RGB相机动捕系列：相机标定（二、标定计算）<br>一、概述</strong></p><p>在上一篇博客中我们讲了相机标定的前置步骤，特征提取，本篇讲述后续的多相机标定中的标定计算部分；主要包括：1、匹配关联；2、几何验证；3、全局优化</p><h1 id="二、标定计算"><a href="#二、标定计算" class="headerlink" title="二、标定计算"></a><strong>二、标定计算</strong></h1><h2 id="2-1、匹配关联"><a href="#2-1、匹配关联" class="headerlink" title="2.1、匹配关联"></a><strong>2.1、匹配关联</strong></h2><p>红点提取完成后，可能存在同一时刻同一相机提取出了多个红点或者未提取到红点，则认为该帧为无效帧，将该帧删除，最后将有效帧进行两两匹配，构建相机-特征匹配关系。</p><h2 id="2-2、几何验证"><a href="#2-2、几何验证" class="headerlink" title="2.2、几何验证"></a><strong>2.2、几何验证</strong></h2><p>匹配关系构建完成后，需要去除误匹配，通过求解基础矩阵F和本质矩阵E进行几何验证（最后一次使用E矩阵），具体方法：首先根据2.1中得到的匹配关系求解基础矩阵F,常见的使用8点法求解基础矩阵，但由于噪声、误匹配等原因使用8点法求解基础矩阵不稳定，由于环境是人在动捕室中持续挥杆采集，以30帧的速度采集4分钟有7200帧，在实际使用中采集4分钟后经过上述2.1的匹配关联后最小的匹配对数量也大于500对，所以由于数据量充足，采用RANSAC（随机采样一致性）对基础矩阵进行估计，可以有效降低误匹配带来的误差。</p><h2 id="2-3、全局优化"><a href="#2-3、全局优化" class="headerlink" title="2.3、全局优化"></a><strong>2.3、全局优化</strong></h2><p>随着图像的不断增加，误差会不断累积，导致最后误差过大，将上述2.2中经过几何验证后的点进行全局优化消除误差。几何验证的匹配点对确保都是正确匹配，假设有N帧图像系列（每帧图像系由M个相机采集到的图像组成，假设每帧图像系列都有多个视图采集才标记点），于是最后的优化方程可以写成：</p><p>当摄像机i观察到轨迹j的时候Wij取1，反之取0，</p><p>||qij- P (C<sub>i</sub>, X<sub>j</sub>)||就是摄像机i中的轨迹j的投影误差累积和。</p><p>P(C<sub>i</sub>, X<sub>j</sub>),表示第j个三维点投影到相机i；</p><p>最优优化的输出结果包括N个三维点的坐标，以及M个相机的内外参数；</p><p>全局优化完成后，通过多次迭代上述2.3过程精确求解出相机的内外参。具体算法可以使用openMVG库实现。</p><p>下面是使用5相机标定的结果和报告，可以看出标定精度非常高，误差为0.8像素。</p><p>标定结果如图：</p><p>标定报告：</p><p>AC_RANSAC参考：<a href="http://www.ipol.im/pub/art/2012/mmm-oh/">http://www.ipol.im/pub/art/2012/mmm-oh/</a></p><p>SFM参考：<a href="https://github.com/openMVG/openMVG/wiki">https://github.com/openMVG/openMVG/wiki</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>RGB相机动捕系列 相机标定</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RGB相机动捕系列：相机标定（一、特征提取）</title>
    <link href="/2024/03/10/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%80%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%89/"/>
    <url>/2024/03/10/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%80%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p><strong>RGB相机动捕系列：相机标定（一、特征提取）<br>一、概述</strong></p><p>在动捕行业中，动捕系统一般采用红外相机捕捉演员身上的mark红外反光点，如vicon、OptiTrack等相机，相机厂商会有配对的相机标定系统。但是红外光学动捕有时会有一定的局限性，比如对于手指的捕捉效果很差，所以有一套RGB相机动捕是很有必要的。但是市面上大多数RGB工业相机并没有配套的相机阵标定系统，本系列文章将阐述RGB相机动捕的整个系统，包括相机标定、相机配准、RGB相机身体动捕、RGB相机手指动捕。本文讲述RGB相机标定中的特征提取方法。</p><h1 id="二、相机标定"><a href="#二、相机标定" class="headerlink" title="二、相机标定"></a><strong>二、相机标定</strong></h1><h2 id="2-1、传统标定方法"><a href="#2-1、传统标定方法" class="headerlink" title="2.1、传统标定方法"></a><strong>2.1、传统标定方法</strong></h2><p>1、使用棋盘标定板进行标定，该方法首先对单个相机的内参进行标定，再对相机组的外参求解，使用该方法只能每次两两相机进行标定， 一般动捕室相机非常多，各相机之间的相对位姿关系也较为复杂，使用该方法需要消耗 大量人力物力，且该方法算法时间长，不利于使用。</p><p>2、使用显著标志物进行标定，该方法在场景中放置显著标志物，通过提取标志物的特征点进行匹配，实际使用中需要设计多个不同的标志物，标志物设计较为困难，特征提取受噪声影响较大，且标志物的放置需要有一定规则，需要保证同一标志物至少两相机可见，对于非专业人员操作不太友好。</p><h2 id="2-2、使用的标定方法"><a href="#2-2、使用的标定方法" class="headerlink" title="2.2、使用的标定方法"></a><strong>2.2、使用的标定方法</strong></h2><p>设计一个顶端使用 LED 红点光源的标定杆。标定时只需要人手持标定杆在动捕室进行挥动，相机通过波形发生器进行同步，通过算法分析实时提取出点光源在各个相机中的帧号和坐标，提取完成后再进行匹配关联，几何验证，最后进行全局优化，即可将相机内参外参一起标定完成，整个标定过程只需要几分钟就能完成大型动捕室相机阵列的高精度标定。</p><h2 id="2-3、特征提取"><a href="#2-3、特征提取" class="headerlink" title="2.3、特征提取"></a><strong>2.3、特征提取</strong></h2><p>该方法通过 LED 红灯作为标定杆，标定杆如下图所示：</p><img src="/2024/03/10/RGB%E7%9B%B8%E6%9C%BA%E5%8A%A8%E6%8D%95%E7%B3%BB%E5%88%97%EF%BC%9A%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%EF%BC%88%E4%B8%80%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%89/1.png" class=""><p>由操作人员手持标定杆在动捕室运动（运动规则是让尽可能多的点均匀的落在不同 相机上，这点很容易做到），通过红点提取算法计算出每帧图像中红点在各相机中的坐标，具体的红点检测算法如下： 特征提取：算法采用自定义特征：红率∙灰度，对于图像中的每个 RGB 像素值，特征 F 的计算公式为：</p><p>F&#x3D;reluR-G+relu(R-B)R+G+B+ε∙(0.3R+0.59G+0.11B)              （1）</p><p>其中𝜺是一个极小值，避免除0。 通过该自定义特征，能够很好的将红点特征提取出来，并且对于噪声也有很好的过滤。特征提取完成之后，通过 DOG（高斯函数的差分）算子对特征进一步处理，提取出点位置，图像中的高斯核函数 G 如下：</p><p>G（x,y,σ）&#x3D;12πσ2e(-x2+y22σ2)                                 （2）</p><p>其中𝝈是方差，代表斑点的尺度。 高斯差分结果 D 的计算公式为：</p><p>D（x,y,σ）&#x3D;G（x,y,σ+δ）-G（x,y,σ-δ）*F(x,y)                （3）</p><p>其中 G 为公式（2）中的高斯核函数，δ是一个极小量，用于构建尺度差分，在实际使用中取10−1，*是卷积运算，F 为公式（1）中的特征图。最后通过阈值判断当差分结果大于阈值时认为该点是红点.公式如下所示：</p><p>R&#x3D;0,  &amp;D（x,y,σ）&lt;threshold1,  &amp;D（x,y,σ）≥threshold                             （4） </p>]]></content>
    
    
    
    <tags>
      
      <tag>RGB相机动捕系列 相机标定</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorRT+CUDA加速人体关键点检测</title>
    <link href="/2022/06/10/TensorRT&amp;CUDA/"/>
    <url>/2022/06/10/TensorRT&amp;CUDA/</url>
    
    <content type="html"><![CDATA[<p>TensorRT+CUDA加速人体关键点检测</p><h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ul><li><a href="#TensorRTCUDA_0">TensorRT+CUDA加速人体关键点检测</a></li><li><a href="#1_09_3">1、 人体关键点检测简介</a></li><li><a href="#2_09CPUGPU_17">2、 CPU和GPU的异同</a></li><li><a href="#3_09TensorRT_23">3、 TensorRT推理加速</a></li><li><a href="#4_09CUDA_42">4、 CUDA编程加速后处理</a></li></ul><h2 id="1、-人体关键点检测简介"><a href="#1、-人体关键点检测简介" class="headerlink" title="1、 人体关键点检测简介"></a>1、 人体关键点检测简介</h2><p>在影视游戏领域中，动画和角色中必须要有动作如奔跑、打斗、跳跃等，动捕技术出来之前的做法是动画师手工一帧帧修，此方法非常耗时，且非常考验动画师的经验。动捕技术常使用演员表演某些动作，并将这些动作转化为数字模型的动作，目前比较主流的几种动捕技术有红外动捕（标记点动捕）：演员穿着紧身衣，衣服上挂满红外反光点，通过红外相机获取反光点的运动轨迹来记录运动数据；惯性动捕：演员身上穿上各种陀螺仪进行记录运动数据；无标记动捕：不使用紧身衣，直接使用rgb相机获取图像，从图像中提取关节点来记录运动数据。上述几种方法各有优缺点，无标记动捕的优势在于演员不需要穿戴各种设备，能随意运动。  </p><img src="/2022/06/10/TensorRT&CUDA/1.png" class=""><img src="/2022/06/10/TensorRT&CUDA/2.png" class=""><img src="/2022/06/10/TensorRT&CUDA/3.png" class=""><p>使用无标记动捕中的第一个重要的步骤就是人体关键点检测，通过算法分析计算出图像中人体各关键点的位置。目前coco人体关键点数据集将人体关键点表示为17个关节，分别是鼻子、左右眼，左右耳，左右肩，左右肘，左右腕，左右臀，左右膝，左右脚踝。如下图所示  </p><img src="/2022/06/10/TensorRT&CUDA/4.png" class=""><img src="/2022/06/10/TensorRT&CUDA/5.png" class=""><p>目前主流的是使用深度学习方法计算出人体关键点位置，但是直接使用神经网络训练出的模型进行计算不够快，不能达到实时效果。由于人体关键点检测是整个无标记动捕过程中的第一步，它的效率直接影响整个流程的实时性，而实时是重要的，无论是作为表演时导演观察演员动作是否合理还是作为演员自身对动作细节的把握，都需要实时观察到整个动作的效果。所以得对整个模型进行加速，一种简单的做法是直接增加显卡的数量和计算能力，但此方法得耗费巨大资源购买显卡。另一种方法则是在软件层面上加速算法处理速度。可以使用了TensorRT和CUDA对整个运算过程进行加速，而这两种方法都是在GPU上进行运算，我们先来看看GPU和CPU有什么不同。</p><h2 id="2、-CPU和GPU的异同"><a href="#2、-CPU和GPU的异同" class="headerlink" title="2、 CPU和GPU的异同"></a>2、 CPU和GPU的异同</h2><p>GPU是graphics procession unit 的缩写，意味图形处理器，与之对应的一个概念是CPU，即center procession unit（中央处理器），一块典型的CPU拥有少数几个快速的计算核心，而一个典型的GPU拥有几百到几千个不那么快速的计算核心，如下图，黄色是控制器，绿色是算数逻辑单元，橙色是存储器，CPU和GPU之间通过PCIe总线连接通信。  </p><img src="/2022/06/10/TensorRT&CUDA/6.png" class=""><p>之所以如此设计，是由于其对应于不同的应用场景，CPU需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断引入大量的分支跳转和中断处理，所以CPU的更多晶体管用于数据缓存和流程控制。而GPU则是用于处理类型高度统一，相互无依存的大规模数据，所以更多的晶体管用于算数逻辑单元。</p><h2 id="3、-TensorRT推理加速"><a href="#3、-TensorRT推理加速" class="headerlink" title="3、 TensorRT推理加速"></a>3、 TensorRT推理加速</h2><p>tensorRT是基于python、C++、CUDA编写的一个库，其核心代码为C++和CUDA，用于优化经过训练的的深度学习模型以实现高性能推理。具体流程是将训练好的网络直接扔到tensorRT中，不再依赖深度学习框架（caffe、TensorFlow、pytorch等），如下图所示：  </p><img src="/2022/06/10/TensorRT&CUDA/7.png" class=""><p>tensorRT的性能优化主要有以下方面：  </p><img src="/2022/06/10/TensorRT&CUDA/8.png" class=""><p>权重激活与精度校准：大部分深度学习框架在训练神经网络时网络中的张量都是32位浮点型的，一旦网络训练完成，在部署推理的过程中不需要反向传播，完全可以适当降低数据的精度（如降低到INT8，相比于float32内存减少为1&#x2F;4），从而提高整体推理速度。虽说降低了数据的精度可能会导致推理结果出现偏差，但是只谈读毒性不谈剂量都是耍流氓，从实际测试中看int8和float差距并不大，可以满足大多数使用。<br>层与张量融合：制约计算速度的CUDA核心计算张量，往往大量的时间也是浪费在CUDA核心的启动和对每一层输入&#x2F;输出张量的读写操作上面，这造成了内存带宽的瓶颈和GPU资源的浪费。TensorRT通过对层间的横向或纵向合并，使得层的数量大大减少，这样就可以一定程度的减少kernel launches和内存读写。  </p><img src="/2022/06/10/TensorRT&CUDA/9.png" class=""><p>横向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心，纵向合并可以把结构相同但权值不同的层和并成一个更宽的层，也只占用一个CUDA核心，对于多分支合并情况，tensorRT完全可以实现直接到需要的地方，不做专门的concat处理，所以这一层也可以取消掉，合并之后如上图中的右边结构，层次更少、占用的CUDA的核心数也更少，因此整个模型结构会更小更快。<br>内核自动调整：根据不同的显卡结构、计算单元数量、内核频率等选择不同的优化策略以及计算方式，寻找最合适当前架构的计算方式，这种优化方法也带来了一定的缺点，经过优化的模型和GPU已经产生了绑定，在不同的GPU上优化的模型并不能兼容，但瑕不掩瑜，一般来说在动捕中整个框架都是事先确认好的，而额外增加或者跟换显卡设备也可以快速的从新推理生成模型。<br>动态张量显存：显存的开辟和释放比较耗时，tensorRT通过一些策略减少显存的开辟和释放次数，增加显存重复利用率从而减少时间。<br>多流执行：使用CUDA中的stream操作，可以并行处理数据，从而提高显卡的利用率。<br>通过将训练好的模型进行tensorRT加速优化，并序列化到磁盘中，使用时通过反序列化即可使用优化后的推理引擎加速。使用tensorRT加速的效果如何，通过对比pytorch和tensorRT，均在1080Ti显卡上执行，在pytorch中的计算时间为55毫秒，使用tensorRT加速后的计算时间为10.5毫秒，速度提高了5.23倍。</p><h2 id="4、-CUDA编程加速后处理"><a href="#4、-CUDA编程加速后处理" class="headerlink" title="4、 CUDA编程加速后处理"></a>4、 CUDA编程加速后处理</h2><p>通过tensorRT优化加速后整个流程并没有结束。首先来看一下关键点回归ground truth的构建问题，主要有coordinate和heatmap两种。Coordinate直接将关键点的坐标作为网络最后的回归目标，使用coordinate之后则不需要再做后续处理，直接将坐标输出给后续流程，但是coordinate回归的是关键点对于图片的offset，而长距离的offset在实际学习过程中很难回归，会造成较大的误差，且在训练过程中提供的监督信息较少，网络收敛较慢。使用heatmap将每一个坐标用一个概率图来表示，对于图片中每个像素都给定一个概率，表示该点属于对应类别关键点的概率，距离关键点越近的概率越趋近1，否则越趋近0，每一个点都设置了监督信息，收敛较快，且对每个像素都提供了预测能有效提高精度。使用heatmap输出的结果并不是单一的坐标，而是一张图像像，如下图所示，整个图像的宽高和输入尺寸一致，17个关键点则有17个维度，需要从这些图像中提取坐标使用CPU计算非常耗时，现在没有了tensorRT这神器帮我们优化了，那么就得靠自己手撸CUDA代码。</p><img src="/2022/06/10/TensorRT&CUDA/10.png" class=""><p>CUDA是NVIDIA公司发布的一种操作GPU计算的硬件和软件架构，下图是CUDA的软件架构，开发应用程序以主机（CPU）为出发点，应用程序可以调用CUDA运行时API、CUDA驱动API以及一些已有的CUDA库。所有这些调用都将利用设备（GPU）的硬件资源。  </p><img src="/2022/06/10/TensorRT&CUDA/11.png" class=""><p>CUDA编程是一个异构模型，不能单独在GPU中运行，必须在和CPU协同工作，在CUDA中有host（CPU及其内存）和device（GPU及其内存）两个重要概念，前面介绍了CPU和GPU的异同，所以对于较多逻辑判断、分支跳转时使用CPU，需要处理大量简单重复的数据使用GPU，并通过PCIe总线进行通信。以处理关键点heatmap为例，第一步需要将heatmap缩放到输入尺寸，具体流程：（1）、得到tensorRT推理之后的数据，由于tensorRT推理后的数据本身就存在显存中，所以不需要再重新复制，直接可以将数据指针传入到处理函数中；（2）、传入之后根据缩放图像大小开辟等量的数据，在cuda中使用&lt;&lt;&lt;grid,block&gt;&gt;&gt;配置线程数。（3）、每个线程单独处理每个像素值计算出当前位置缩放的值，使程序在逻辑上处于并行状态。<br>上面步骤中grid表示线程块数，block表示每个线程块内的线程数，所以总大小为grid*block个线程， 之所以说在逻辑上是并行的，是由于GPU和CPU类似，多线程如果没有多核支持，在物理层上无法实现并行。但GPU有很多CUDA核心，GPU硬件的一个核心组件是SM(流式多处理器)，SM的核心组件包括CUDA核心、共享内存、寄存器等，SM可以并行的执行数百个线程。所以，上述流程被执行时，grid中的线程块被分配到SM上，SM中以线程束作为基本执行单元，目前所以GPU中线程束大小都是32，所以物理上并行的最大线程数为32个线程。<br>Heatmap经过缩放，非极大值抑制等一系列处理最终输出得到关键点精确坐标，对比CUDA编程和CPU编程，在CPU（i7 3.2GHz）上处理整个过程消耗25毫秒，在CUDA（1080Ti显卡）中处理消耗2.3毫秒,提升超过了10倍。</p><p>注。<a href="#fn1">1</a></p><p>本文仅供学习参考，未经运行，拒绝转载或者用作他用。</p><hr><ol><li>参考连接<br><a href="https://developer.nvidia.com/zh-cn/tensorrt">https://developer.nvidia.com/zh-cn/tensorrt</a><br><a href="https://zhuanlan.zhihu.com/p/102457223">https://zhuanlan.zhihu.com/p/102457223</a><br><a href="https://zhuanlan.zhihu.com/p/69042249">https://zhuanlan.zhihu.com/p/69042249</a><br><a href="https://zhuanlan.zhihu.com/p/371663018">https://zhuanlan.zhihu.com/p/371663018</a> <a href="#fnref1">↩︎</a>## 目标</li></ol><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3>]]></content>
    
    
    
    <tags>
      
      <tag>TensorRT CUDA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2021/06/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2021/06/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><img src="/2021/06/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.png" class="">]]></content>
    
    
    
    <tags>
      
      <tag>Test</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
